{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from utils import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will adopt the `generate_text_simple` function from the previous chapter and introduce 2 new functions.\n",
    "- `text_to_token_ids` and `token_ids_to_text`. These functions will help the conversion between text and token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from utils import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: Every effort moves you rentingetic wasnم refres RexAngel infieldcigans\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "print(f\"Output Text: {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output is gibberish\n",
    "- We will implement a numerical method to evaluate the generated content. This will allow is to monitor and enhance the model's performance throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS:\n",
      " tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n",
      "OUTPUTS:\n",
      " tensor([[ 3626,  6100,   345],\n",
      "        [ 1107,   588, 11311]])\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "inputs = []\n",
    "txt1 = \"every effort moves\"\n",
    "txt2 = \"I really like\"\n",
    "\n",
    "inputs.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "inputs.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "inputs = torch.stack(inputs, dim=0)\n",
    "print(\"INPUTS:\\n\", inputs)\n",
    "\n",
    "\n",
    "#TARGETS\n",
    "targets = []\n",
    "txt1 = \" effort moves you\"\n",
    "txt2 = \" really like chocolate\"\n",
    "\n",
    "targets.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "targets.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "targets = torch.stack(targets, dim=0)\n",
    "print(\"OUTPUTS:\\n\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Lets get the logits\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN IDS:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"TOKEN IDS:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.2671e-05, 3.1046e-05, 1.1696e-05])\n",
      "Text 2: tensor([1.0426e-05, 5.4604e-05, 4.7716e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above are the probabilities of the each input token in the above 2 text examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation\n",
    "- Used to update the model weights\n",
    "- Compute the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5296, -10.3800, -11.3563, -11.4712,  -9.8154, -12.2528])\n"
     ]
    }
   ],
   "source": [
    "# Calculate log of each token across the entire batch\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8009)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average of the log\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8009)\n"
     ]
    }
   ],
   "source": [
    "# Multiply by negative 1\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This process performed above is known as cross-entropy loss.\n",
    "- Pytorch has an in-built function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logits are 3-D (batch_size, number of tokens, vocab-size)\n",
    "- The targets tensor has 2-D (batch_size, number of tokens)\n",
    "- For the cross_entropy we have to flatten by combining them over the batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8009)\n"
     ]
    }
   ],
   "source": [
    "# Using pytorch to calculate the loss\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "- Its a measure that is often used to evaluate the performance of models in tasks like language modeling.\n",
    "- Provides a more interpretable way to understand the uncertainity of model in predicting the next token in a sequence.\n",
    "- Perplexity measures how well the probability distribution predicted by the model matches the actual distribution of words in the dataset. Similar to loss, a lower perplexity indicated that the model predictions are closer to the actual distribution.\n",
    "- It signifies the effective vocabulary size about which the model is uncertain at each step.\n",
    "- This would translate to the model being unsure about which among all the tokens in the vocabulary to generate as the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49064.1680)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the above excercise we have computed the loss and perplexity to 2 inputs. But we will now extend it to the entire training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the training and validation set losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first prepare the training and validation datasets.\n",
    "# We will use the same dataset we had used in earlier chapter (\"The Verdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/chapter2/the-verdict.txt'\n",
    "# file_path = 'data/tinystories.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters: 20479\n",
      "Number of Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"Number of Characters: {total_characters}\")\n",
    "print(f\"Number of Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will split the data in to training and validation sets.\n",
    "- They will be tokenized\n",
    "- Sample will be generated of `context_length`\n",
    "- Samples will be grouped to form batches (with shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.95\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now create the data loader with the datasets we have above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(123)\n",
    "\n",
    "# train_loader = create_dataloader_v1(\n",
    "#     file_path=\"data/tinystories_train.txt\",\n",
    "#     batch_size=256,  # Increased for better GPU utilization\n",
    "#     max_length=GPT_CONFIG_124M['context_length'],\n",
    "#     stride=GPT_CONFIG_124M['context_length'],\n",
    "#     drop_last=True,\n",
    "#     shuffle=True,\n",
    "#     num_workers=4,    # Parallel loading\n",
    "#     pin_memory=True   # Faster GPU transfer\n",
    "# )\n",
    "\n",
    "# # For validation, you can use a separate file or create a custom split\n",
    "# val_loader = create_dataloader_v2(\n",
    "#     file_path=\"data/tinystories_val.txt\",\n",
    "#     batch_size=256,\n",
    "#     max_length=GPT_CONFIG_124M['context_length'],\n",
    "#     stride=GPT_CONFIG_124M['context_length'],\n",
    "#     drop_last=True,\n",
    "#     shuffle=True,\n",
    "#     num_workers=4,\n",
    "#     pin_memory=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "with open(\"data/tinystories_train_small.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_text = f.read()\n",
    "\n",
    "with open(\"data/tinystories_val_small.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloader_v1\n",
    "train_loader = create_dataloader_v1(\n",
    "    txt=train_text,\n",
    "    batch_size=64,\n",
    "    max_length=GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length'],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=1,          # Reduce workers for memory stability\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    txt=val_text,\n",
    "    batch_size=128,\n",
    "    max_length=GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length'],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Train loader:\")\n",
    "# for x, y in train_loader:\n",
    "#     print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nValidation loader:\")\n",
    "# for x, y in val_loader:\n",
    "#     print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device): # This is only for a batch\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute over all the batches\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1657/1657 [06:54<00:00,  3.99it/s]\n",
      "Validation loss: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:22<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.958219230067105\n",
      "Validation loss: 10.957235757694688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "from tqdm import tqdm\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(tqdm(train_loader, desc=\"Training loss\"), model, device)\n",
    "    val_loss = calc_loss_loader(tqdm(val_loader, desc=\"Validation loss\"), model, device)\n",
    "\n",
    "print(f\"Training loss: {train_loss}\")\n",
    "print(f\"Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Ep 1 (Step 000000): Train Loss 10.959\n",
      "Ep 1 (Step 000000): Train Loss 9.346 Val loss 9.346\n",
      "Ep 1 (Step 000100): Train Loss 3.629\n",
      "Ep 1 (Step 000200): Train Loss 3.134\n",
      "Ep 1 (Step 000300): Train Loss 2.772\n",
      "Ep 1 (Step 000400): Train Loss 2.663\n",
      "Ep 1 (Step 000500): Train Loss 2.495\n",
      "Ep 1 (Step 000600): Train Loss 2.392\n",
      "Ep 1 (Step 000700): Train Loss 2.329\n",
      "Ep 1 (Step 000800): Train Loss 2.277\n",
      "Ep 1 (Step 000900): Train Loss 2.255\n",
      "Ep 1 (Step 001000): Train Loss 2.211\n",
      "Ep 1 (Step 001000): Train Loss 2.112 Val loss 2.147\n",
      "Ep 1 (Step 001100): Train Loss 2.194\n",
      "Ep 1 (Step 001200): Train Loss 2.140\n",
      "Ep 1 (Step 001300): Train Loss 2.091\n",
      "Ep 1 (Step 001400): Train Loss 2.066\n",
      "Ep 1 (Step 001500): Train Loss 1.999\n",
      "Ep 1 (Step 001600): Train Loss 2.069\n",
      "Every effort moves you to me. I love you too. You are my best friend.\" <START><STORY>Once upon a time, there was a little boy named Tim. Tim had a big, red ball. He loved to play with his ball\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from utils import train_model_simple\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 1\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=1000,          # Now interpreted as batches, not steps\n",
    "    eval_iter=50,            # Reduced from 1000 to 10 for memory efficiency\n",
    "    start_context=start_context,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "#     # Initialize list to tack losses and tokens seens\n",
    "#     train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "#     tokens_seen , global_step = 0, -1\n",
    "\n",
    "#     for epoch in range(num_epochs):# start main loop\n",
    "#         model.train() # ensures/set the model into train mode since there are eval step that happen sometimes\n",
    "#         for input_batch, target_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             tokens_seen += input_batch.numel()\n",
    "#             global_step += 1\n",
    "\n",
    "#             if global_step % eval_freq == 0:\n",
    "#                 train_loss, val_loss = evaluate_model(model , train_loader, val_loader, device, eval_iter)\n",
    "#                 train_losses.append(train_loss)\n",
    "#                 val_losses.append(val_loss)\n",
    "#                 track_tokens_seen.append(tokens_seen)\n",
    "#                 print(f\"Ep {epoch+1} (Step {global_step:06d}): Train Loss {train_loss:.3f} Val loss {val_loss:0.3f}\")\n",
    "\n",
    "#         generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "#     return train_losses, val_losses, track_tokens_seen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `evaluate_model` caluclates the losses over the training and validation set while ensuring that the model is in evaluation mode.\n",
    "- This means that gradient checking and dropout are disables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() # set model in eval mode to turn off dropout.\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train() # set the model back to train mode\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval() # set model in eval mode\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
    "\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train() # set model back in train mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: We will use `AdamW` as the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First TRAIN RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "device='cuda'\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 10\n",
    "start_context = \"Every effort moves you\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train Loss 9.798 Val loss 9.899\n",
      "Ep 1 (Step 000005): Train Loss 7.928 Val loss 8.333\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train Loss 6.593 Val loss 7.043\n",
      "Ep 2 (Step 000015): Train Loss 6.006 Val loss 6.601\n",
      "Every effort moves you, the,, the,, the, the,, the, the,,, the,,, the,, the,, of the,, the,,, the, the,, the, the,,,,, of\n",
      "Ep 3 (Step 000020): Train Loss 6.533 Val loss 7.168\n",
      "Ep 3 (Step 000025): Train Loss 5.600 Val loss 6.484\n",
      "Every effort moves you, and I had to the of the----.                                       \n",
      "Ep 4 (Step 000030): Train Loss 5.119 Val loss 6.416\n",
      "Ep 4 (Step 000035): Train Loss 4.621 Val loss 6.341\n",
      "Every effort moves you.       \"- the picture.   \", I had the picture the picture, and I had the the picture-- the picture and I had the picture.          \n",
      "Ep 5 (Step 000040): Train Loss 3.941 Val loss 6.275\n",
      "Every effort moves you know it was his pictures--I was his pictures--I had the.           \"I, and, and his, and, and his, and he was, and the, and he was his\n",
      "Ep 6 (Step 000045): Train Loss 3.352 Val loss 6.225\n",
      "Ep 6 (Step 000050): Train Loss 2.960 Val loss 6.185\n",
      "Every effort moves you know it was not that my dear--I had been--as his painting.                                  \n",
      "Ep 7 (Step 000055): Train Loss 2.471 Val loss 6.197\n",
      "Ep 7 (Step 000060): Train Loss 1.789 Val loss 6.182\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony.   \"Oh, when I looked up, I felt to see a smile behind his pictures.                \n",
      "Ep 8 (Step 000065): Train Loss 1.300 Val loss 6.265\n",
      "Ep 8 (Step 000070): Train Loss 0.939 Val loss 6.263\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train Loss 0.732 Val loss 6.398\n",
      "Ep 9 (Step 000080): Train Loss 0.554 Val loss 6.409\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Ep 10 (Step 000085): Train Loss 0.353 Val loss 6.512\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    device, \n",
    "    num_epochs=num_epochs, \n",
    "    eval_freq=5, \n",
    "    eval_iter=5, \n",
    "    start_context=start_context,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
