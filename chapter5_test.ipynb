{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from utils import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will adopt the `generate_text_simple` function from the previous chapter and introduce 2 new functions.\n",
    "- `text_to_token_ids` and `token_ids_to_text`. These functions will help the conversion between text and token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from utils import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: Every effort moves you rentingetic wasnم refres RexAngel infieldcigans\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "print(f\"Output Text: {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output is gibberish\n",
    "- We will implement a numerical method to evaluate the generated content. This will allow is to monitor and enhance the model's performance throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS:\n",
      " tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n",
      "OUTPUTS:\n",
      " tensor([[ 3626,  6100,   345],\n",
      "        [ 1107,   588, 11311]])\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "inputs = []\n",
    "txt1 = \"every effort moves\"\n",
    "txt2 = \"I really like\"\n",
    "\n",
    "inputs.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "inputs.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "inputs = torch.stack(inputs, dim=0)\n",
    "print(\"INPUTS:\\n\", inputs)\n",
    "\n",
    "\n",
    "#TARGETS\n",
    "targets = []\n",
    "txt1 = \" effort moves you\"\n",
    "txt2 = \" really like chocolate\"\n",
    "\n",
    "targets.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "targets.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "targets = torch.stack(targets, dim=0)\n",
    "print(\"OUTPUTS:\\n\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Lets get the logits\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN IDS:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"TOKEN IDS:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.2671e-05, 3.1046e-05, 1.1696e-05])\n",
      "Text 2: tensor([1.0426e-05, 5.4604e-05, 4.7716e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above are the probabilities of the each input token in the above 2 text examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation\n",
    "- Used to update the model weights\n",
    "- Compute the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5296, -10.3800, -11.3563, -11.4712,  -9.8154, -12.2528])\n"
     ]
    }
   ],
   "source": [
    "# Calculate log of each token across the entire batch\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8009)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average of the log\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8009)\n"
     ]
    }
   ],
   "source": [
    "# Multiply by negative 1\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This process performed above is known as cross-entropy loss.\n",
    "- Pytorch has an in-built function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logits are 3-D (batch_size, number of tokens, vocab-size)\n",
    "- The targets tensor has 2-D (batch_size, number of tokens)\n",
    "- For the cross_entropy we have to flatten by combining them over the batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8009)\n"
     ]
    }
   ],
   "source": [
    "# Using pytorch to calculate the loss\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "- Its a measure that is often used to evaluate the performance of models in tasks like language modeling.\n",
    "- Provides a more interpretable way to understand the uncertainity of model in predicting the next token in a sequence.\n",
    "- Perplexity measures how well the probability distribution predicted by the model matches the actual distribution of words in the dataset. Similar to loss, a lower perplexity indicated that the model predictions are closer to the actual distribution.\n",
    "- It signifies the effective vocabulary size about which the model is uncertain at each step.\n",
    "- This would translate to the model being unsure about which among all the tokens in the vocabulary to generate as the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49064.1641)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the above excercise we have computed the loss and perplexity to 2 inputs. But we will now extend it to the entire training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the training and validation set losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first prepare the training and validation datasets.\n",
    "# We will use the same dataset we had used in earlier chapter (\"The Verdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'data/chapter2/the-verdict.txt'\n",
    "file_path = 'data/chapter5/Frankinstein.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters: 418918\n",
      "Number of Tokens: 101598\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"Number of Characters: {total_characters}\")\n",
    "print(f\"Number of Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will split the data in to training and validation sets.\n",
    "- They will be tokenized\n",
    "- Sample will be generated of `context_length`\n",
    "- Samples will be grouped to form batches (with shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now create the data loader with the datasets we have above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloader_v1\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=5,\n",
    "    max_length=GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length'],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=5,\n",
    "    max_length=GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length'],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loader:\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device): # This is only for a batch\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute over all the batches\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6965724674748702\n",
      "Validation loss: 7.234536852155413\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(f\"Training loss: {train_loss}\")\n",
    "print(f\"Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize list to tack losses and tokens seens\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen , global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):# start main loop\n",
    "        model.train() # ensures/set the model into train mode since there are eval step that happen sometimes\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model , train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): Train Loss {train_loss:.3f} Val loss {val_loss:0.3f}\")\n",
    "\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `evaluate_model` caluclates the losses over the training and validation set while ensuring that the model is in evaluation mode.\n",
    "- This means that gradient checking and dropout are disables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() # set model in eval mode to turn off dropout.\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train() # set the model back to train mode\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval() # set model in eval mode\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
    "\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train() # set model back in train mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: We will use `AdamW` as the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First TRAIN RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "device='mps'\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 10\n",
    "start_context = \"Every effort moves you\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train Loss 9.726 Val loss 9.795\n",
      "Ep 1 (Step 000005): Train Loss 8.209 Val loss 8.252\n",
      "Ep 1 (Step 000010): Train Loss 7.420 Val loss 7.468\n",
      "Ep 1 (Step 000015): Train Loss 7.126 Val loss 7.237\n",
      "Ep 1 (Step 000020): Train Loss 6.784 Val loss 7.099\n",
      "Ep 1 (Step 000025): Train Loss 7.056 Val loss 7.046\n",
      "Ep 1 (Step 000030): Train Loss 6.792 Val loss 6.985\n",
      "Ep 1 (Step 000035): Train Loss 6.834 Val loss 6.961\n",
      "Ep 1 (Step 000040): Train Loss 6.832 Val loss 6.954\n",
      "Ep 1 (Step 000045): Train Loss 6.737 Val loss 6.813\n",
      "Ep 1 (Step 000050): Train Loss 6.370 Val loss 6.701\n",
      "Ep 1 (Step 000055): Train Loss 6.608 Val loss 6.714\n",
      "Ep 1 (Step 000060): Train Loss 6.588 Val loss 6.555\n",
      "Ep 1 (Step 000065): Train Loss 6.426 Val loss 6.639\n",
      "Ep 1 (Step 000070): Train Loss 6.352 Val loss 6.458\n",
      "Ep 1 (Step 000075): Train Loss 6.257 Val loss 6.530\n",
      "Ep 1 (Step 000080): Train Loss 6.221 Val loss 6.378\n",
      "Ep 1 (Step 000085): Train Loss 6.141 Val loss 6.300\n",
      "Ep 1 (Step 000090): Train Loss 6.313 Val loss 6.435\n",
      "Ep 1 (Step 000095): Train Loss 6.340 Val loss 6.314\n",
      "Ep 1 (Step 000100): Train Loss 5.914 Val loss 6.462\n",
      "Ep 1 (Step 000105): Train Loss 6.170 Val loss 6.312\n",
      "Ep 1 (Step 000110): Train Loss 6.221 Val loss 6.379\n",
      "Ep 1 (Step 000115): Train Loss 5.914 Val loss 6.397\n",
      "Ep 1 (Step 000120): Train Loss 5.938 Val loss 6.167\n",
      "Ep 1 (Step 000125): Train Loss 5.954 Val loss 6.297\n",
      "Ep 1 (Step 000130): Train Loss 5.928 Val loss 6.250\n",
      "Ep 1 (Step 000135): Train Loss 5.727 Val loss 6.124\n",
      "Ep 1 (Step 000140): Train Loss 5.868 Val loss 6.058\n",
      "Ep 1 (Step 000145): Train Loss 5.898 Val loss 6.241\n",
      "Ep 1 (Step 000150): Train Loss 5.881 Val loss 6.215\n",
      "Ep 1 (Step 000155): Train Loss 5.887 Val loss 6.204\n",
      "Ep 1 (Step 000160): Train Loss 5.922 Val loss 6.066\n",
      "Ep 1 (Step 000165): Train Loss 5.696 Val loss 6.103\n",
      "Ep 1 (Step 000170): Train Loss 5.743 Val loss 6.135\n",
      "Ep 1 (Step 000175): Train Loss 5.589 Val loss 6.121\n",
      "Every effort moves you, and the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 2 (Step 000180): Train Loss 5.576 Val loss 6.072\n",
      "Ep 2 (Step 000185): Train Loss 5.757 Val loss 6.020\n",
      "Ep 2 (Step 000190): Train Loss 5.781 Val loss 6.007\n",
      "Ep 2 (Step 000195): Train Loss 5.735 Val loss 6.156\n",
      "Ep 2 (Step 000200): Train Loss 5.491 Val loss 5.970\n",
      "Ep 2 (Step 000205): Train Loss 5.608 Val loss 6.167\n",
      "Ep 2 (Step 000210): Train Loss 5.776 Val loss 6.197\n",
      "Ep 2 (Step 000215): Train Loss 5.866 Val loss 6.013\n",
      "Ep 2 (Step 000220): Train Loss 5.667 Val loss 6.100\n",
      "Ep 2 (Step 000225): Train Loss 5.584 Val loss 6.077\n",
      "Ep 2 (Step 000230): Train Loss 5.760 Val loss 6.107\n",
      "Ep 2 (Step 000235): Train Loss 5.577 Val loss 6.107\n",
      "Ep 2 (Step 000240): Train Loss 5.523 Val loss 6.059\n",
      "Ep 2 (Step 000245): Train Loss 5.614 Val loss 6.028\n",
      "Ep 2 (Step 000250): Train Loss 5.622 Val loss 5.970\n",
      "Ep 2 (Step 000255): Train Loss 5.405 Val loss 6.064\n",
      "Ep 2 (Step 000260): Train Loss 5.610 Val loss 5.956\n",
      "Ep 2 (Step 000265): Train Loss 5.460 Val loss 6.091\n",
      "Ep 2 (Step 000270): Train Loss 5.512 Val loss 6.104\n",
      "Ep 2 (Step 000275): Train Loss 5.432 Val loss 6.076\n",
      "Ep 2 (Step 000280): Train Loss 5.401 Val loss 5.955\n",
      "Ep 2 (Step 000285): Train Loss 5.528 Val loss 6.001\n",
      "Ep 2 (Step 000290): Train Loss 5.493 Val loss 6.034\n",
      "Ep 2 (Step 000295): Train Loss 5.285 Val loss 5.904\n",
      "Ep 2 (Step 000300): Train Loss 5.625 Val loss 5.878\n",
      "Ep 2 (Step 000305): Train Loss 5.323 Val loss 6.128\n",
      "Ep 2 (Step 000310): Train Loss 5.360 Val loss 5.901\n",
      "Ep 2 (Step 000315): Train Loss 5.516 Val loss 6.043\n",
      "Ep 2 (Step 000320): Train Loss 5.545 Val loss 5.964\n",
      "Ep 2 (Step 000325): Train Loss 5.400 Val loss 6.005\n",
      "Ep 2 (Step 000330): Train Loss 5.400 Val loss 5.990\n",
      "Ep 2 (Step 000335): Train Loss 5.307 Val loss 5.986\n",
      "Ep 2 (Step 000340): Train Loss 5.472 Val loss 5.929\n",
      "Ep 2 (Step 000345): Train Loss 5.365 Val loss 6.024\n",
      "Ep 2 (Step 000350): Train Loss 5.345 Val loss 5.880\n",
      "Ep 2 (Step 000355): Train Loss 5.104 Val loss 5.907\n",
      "Every effort moves you       ““““““““” ” said I had “““” said I had “““\n",
      "Ep 3 (Step 000360): Train Loss 5.296 Val loss 5.971\n",
      "Ep 3 (Step 000365): Train Loss 5.224 Val loss 5.973\n",
      "Ep 3 (Step 000370): Train Loss 5.180 Val loss 5.902\n",
      "Ep 3 (Step 000375): Train Loss 5.327 Val loss 6.007\n",
      "Ep 3 (Step 000380): Train Loss 5.199 Val loss 5.854\n",
      "Ep 3 (Step 000385): Train Loss 5.352 Val loss 5.839\n",
      "Ep 3 (Step 000390): Train Loss 5.106 Val loss 5.973\n",
      "Ep 3 (Step 000395): Train Loss 5.164 Val loss 5.906\n",
      "Ep 3 (Step 000400): Train Loss 5.183 Val loss 6.013\n",
      "Ep 3 (Step 000405): Train Loss 5.210 Val loss 5.973\n",
      "Ep 3 (Step 000410): Train Loss 5.195 Val loss 5.905\n",
      "Ep 3 (Step 000415): Train Loss 5.103 Val loss 5.852\n",
      "Ep 3 (Step 000420): Train Loss 5.328 Val loss 5.904\n",
      "Ep 3 (Step 000425): Train Loss 4.950 Val loss 5.905\n",
      "Ep 3 (Step 000430): Train Loss 5.212 Val loss 5.917\n",
      "Ep 3 (Step 000435): Train Loss 4.862 Val loss 5.833\n",
      "Ep 3 (Step 000440): Train Loss 4.954 Val loss 5.898\n",
      "Ep 3 (Step 000445): Train Loss 5.092 Val loss 5.866\n",
      "Ep 3 (Step 000450): Train Loss 4.960 Val loss 5.759\n",
      "Ep 3 (Step 000455): Train Loss 4.906 Val loss 5.892\n",
      "Ep 3 (Step 000460): Train Loss 5.166 Val loss 5.877\n",
      "Ep 3 (Step 000465): Train Loss 5.036 Val loss 5.998\n",
      "Ep 3 (Step 000470): Train Loss 5.073 Val loss 5.825\n",
      "Ep 3 (Step 000475): Train Loss 4.975 Val loss 5.898\n",
      "Ep 3 (Step 000480): Train Loss 4.881 Val loss 5.840\n",
      "Ep 3 (Step 000485): Train Loss 4.982 Val loss 5.921\n",
      "Ep 3 (Step 000490): Train Loss 4.919 Val loss 5.792\n",
      "Ep 3 (Step 000495): Train Loss 4.992 Val loss 5.843\n",
      "Ep 3 (Step 000500): Train Loss 5.020 Val loss 5.801\n",
      "Ep 3 (Step 000505): Train Loss 5.070 Val loss 5.834\n",
      "Ep 3 (Step 000510): Train Loss 4.861 Val loss 5.823\n",
      "Ep 3 (Step 000515): Train Loss 4.941 Val loss 5.782\n",
      "Ep 3 (Step 000520): Train Loss 4.850 Val loss 5.867\n",
      "Ep 3 (Step 000525): Train Loss 4.839 Val loss 5.786\n",
      "Ep 3 (Step 000530): Train Loss 4.852 Val loss 5.978\n",
      "Every effort moves you are not                     and the same the   and the same, and the              \n",
      "Ep 4 (Step 000535): Train Loss 4.904 Val loss 5.826\n",
      "Ep 4 (Step 000540): Train Loss 4.773 Val loss 5.820\n",
      "Ep 4 (Step 000545): Train Loss 4.855 Val loss 5.872\n",
      "Ep 4 (Step 000550): Train Loss 4.784 Val loss 5.876\n",
      "Ep 4 (Step 000555): Train Loss 4.939 Val loss 5.930\n",
      "Ep 4 (Step 000560): Train Loss 4.639 Val loss 5.915\n",
      "Ep 4 (Step 000565): Train Loss 4.788 Val loss 6.006\n",
      "Ep 4 (Step 000570): Train Loss 4.694 Val loss 5.918\n",
      "Ep 4 (Step 000575): Train Loss 4.698 Val loss 5.827\n",
      "Ep 4 (Step 000580): Train Loss 4.635 Val loss 5.993\n",
      "Ep 4 (Step 000585): Train Loss 4.585 Val loss 5.896\n",
      "Ep 4 (Step 000590): Train Loss 4.828 Val loss 5.775\n",
      "Ep 4 (Step 000595): Train Loss 4.770 Val loss 5.839\n",
      "Ep 4 (Step 000600): Train Loss 4.756 Val loss 5.885\n",
      "Ep 4 (Step 000605): Train Loss 4.568 Val loss 5.800\n",
      "Ep 4 (Step 000610): Train Loss 4.652 Val loss 5.991\n",
      "Ep 4 (Step 000615): Train Loss 4.793 Val loss 5.853\n",
      "Ep 4 (Step 000620): Train Loss 4.705 Val loss 5.861\n",
      "Ep 4 (Step 000625): Train Loss 4.295 Val loss 5.949\n",
      "Ep 4 (Step 000630): Train Loss 4.781 Val loss 5.832\n",
      "Ep 4 (Step 000635): Train Loss 4.558 Val loss 6.008\n",
      "Ep 4 (Step 000640): Train Loss 4.404 Val loss 5.833\n",
      "Ep 4 (Step 000645): Train Loss 4.478 Val loss 5.886\n",
      "Ep 4 (Step 000650): Train Loss 4.679 Val loss 5.868\n",
      "Ep 4 (Step 000655): Train Loss 4.595 Val loss 5.859\n",
      "Ep 4 (Step 000660): Train Loss 4.659 Val loss 5.901\n",
      "Ep 4 (Step 000665): Train Loss 4.350 Val loss 5.831\n",
      "Ep 4 (Step 000670): Train Loss 4.294 Val loss 5.824\n",
      "Ep 4 (Step 000675): Train Loss 4.531 Val loss 5.794\n",
      "Ep 4 (Step 000680): Train Loss 4.463 Val loss 5.785\n",
      "Ep 4 (Step 000685): Train Loss 4.321 Val loss 5.740\n",
      "Ep 4 (Step 000690): Train Loss 4.442 Val loss 5.685\n",
      "Ep 4 (Step 000695): Train Loss 4.651 Val loss 5.788\n",
      "Ep 4 (Step 000700): Train Loss 4.505 Val loss 5.812\n",
      "Ep 4 (Step 000705): Train Loss 4.204 Val loss 5.760\n",
      "Ep 4 (Step 000710): Train Loss 4.526 Val loss 5.872\n",
      "Every effort moves you   and the and to the and the and the and the and the and the and the and the and the and the and the and the and the and the and\n",
      "Ep 5 (Step 000715): Train Loss 4.433 Val loss 5.848\n",
      "Ep 5 (Step 000720): Train Loss 4.259 Val loss 5.845\n",
      "Ep 5 (Step 000725): Train Loss 4.387 Val loss 5.784\n",
      "Ep 5 (Step 000730): Train Loss 4.278 Val loss 5.724\n",
      "Ep 5 (Step 000735): Train Loss 4.122 Val loss 5.829\n",
      "Ep 5 (Step 000740): Train Loss 4.123 Val loss 5.863\n",
      "Ep 5 (Step 000745): Train Loss 4.101 Val loss 5.775\n",
      "Ep 5 (Step 000750): Train Loss 4.145 Val loss 5.962\n",
      "Ep 5 (Step 000755): Train Loss 4.056 Val loss 5.919\n",
      "Ep 5 (Step 000760): Train Loss 3.893 Val loss 5.937\n",
      "Ep 5 (Step 000765): Train Loss 3.889 Val loss 5.979\n",
      "Ep 5 (Step 000770): Train Loss 4.336 Val loss 5.864\n",
      "Ep 5 (Step 000775): Train Loss 3.962 Val loss 5.943\n",
      "Ep 5 (Step 000780): Train Loss 3.948 Val loss 5.987\n",
      "Ep 5 (Step 000785): Train Loss 4.059 Val loss 6.014\n",
      "Ep 5 (Step 000790): Train Loss 4.133 Val loss 5.915\n",
      "Ep 5 (Step 000795): Train Loss 4.137 Val loss 5.916\n",
      "Ep 5 (Step 000800): Train Loss 3.931 Val loss 5.989\n",
      "Ep 5 (Step 000805): Train Loss 3.856 Val loss 5.861\n",
      "Ep 5 (Step 000810): Train Loss 4.273 Val loss 5.940\n",
      "Ep 5 (Step 000815): Train Loss 4.276 Val loss 5.878\n",
      "Ep 5 (Step 000820): Train Loss 3.835 Val loss 5.991\n",
      "Ep 5 (Step 000825): Train Loss 3.827 Val loss 5.852\n",
      "Ep 5 (Step 000830): Train Loss 4.012 Val loss 5.865\n",
      "Ep 5 (Step 000835): Train Loss 3.938 Val loss 5.959\n",
      "Ep 5 (Step 000840): Train Loss 4.032 Val loss 5.974\n",
      "Ep 5 (Step 000845): Train Loss 3.979 Val loss 6.004\n",
      "Ep 5 (Step 000850): Train Loss 3.848 Val loss 5.865\n",
      "Ep 5 (Step 000855): Train Loss 3.687 Val loss 5.991\n",
      "Ep 5 (Step 000860): Train Loss 3.924 Val loss 5.902\n",
      "Ep 5 (Step 000865): Train Loss 3.697 Val loss 5.904\n",
      "Ep 5 (Step 000870): Train Loss 3.769 Val loss 6.025\n",
      "Ep 5 (Step 000875): Train Loss 3.742 Val loss 5.938\n",
      "Ep 5 (Step 000880): Train Loss 3.881 Val loss 5.958\n",
      "Ep 5 (Step 000885): Train Loss 3.883 Val loss 5.845\n",
      "Every effort moves you; I  and that I had not beheld the and misery of the ’     ““““““““““““�\n",
      "Ep 6 (Step 000890): Train Loss 3.715 Val loss 5.913\n",
      "Ep 6 (Step 000895): Train Loss 3.685 Val loss 5.885\n",
      "Ep 6 (Step 000900): Train Loss 3.710 Val loss 6.059\n",
      "Ep 6 (Step 000905): Train Loss 3.460 Val loss 6.033\n",
      "Ep 6 (Step 000910): Train Loss 3.662 Val loss 6.037\n",
      "Ep 6 (Step 000915): Train Loss 3.625 Val loss 6.025\n",
      "Ep 6 (Step 000920): Train Loss 3.613 Val loss 6.036\n",
      "Ep 6 (Step 000925): Train Loss 3.533 Val loss 6.049\n",
      "Ep 6 (Step 000930): Train Loss 3.399 Val loss 6.131\n",
      "Ep 6 (Step 000935): Train Loss 3.364 Val loss 5.996\n",
      "Ep 6 (Step 000940): Train Loss 3.749 Val loss 6.079\n",
      "Ep 6 (Step 000945): Train Loss 3.555 Val loss 6.056\n",
      "Ep 6 (Step 000950): Train Loss 3.580 Val loss 6.119\n",
      "Ep 6 (Step 000955): Train Loss 3.725 Val loss 6.215\n",
      "Ep 6 (Step 000960): Train Loss 3.450 Val loss 6.214\n",
      "Ep 6 (Step 000965): Train Loss 3.437 Val loss 6.148\n",
      "Ep 6 (Step 000970): Train Loss 3.790 Val loss 6.101\n",
      "Ep 6 (Step 000975): Train Loss 3.437 Val loss 6.112\n",
      "Ep 6 (Step 000980): Train Loss 3.309 Val loss 6.016\n",
      "Ep 6 (Step 000985): Train Loss 3.529 Val loss 6.203\n",
      "Ep 6 (Step 000990): Train Loss 3.378 Val loss 6.138\n",
      "Ep 6 (Step 000995): Train Loss 3.555 Val loss 6.157\n",
      "Ep 6 (Step 001000): Train Loss 2.802 Val loss 5.998\n",
      "Ep 6 (Step 001005): Train Loss 3.088 Val loss 6.281\n",
      "Ep 6 (Step 001010): Train Loss 3.090 Val loss 5.984\n",
      "Ep 6 (Step 001015): Train Loss 3.107 Val loss 6.188\n",
      "Ep 6 (Step 001020): Train Loss 3.294 Val loss 5.985\n",
      "Ep 6 (Step 001025): Train Loss 3.271 Val loss 6.048\n",
      "Ep 6 (Step 001030): Train Loss 3.148 Val loss 6.214\n",
      "Ep 6 (Step 001035): Train Loss 3.023 Val loss 6.222\n",
      "Ep 6 (Step 001040): Train Loss 3.108 Val loss 5.997\n",
      "Ep 6 (Step 001045): Train Loss 3.184 Val loss 6.040\n",
      "Ep 6 (Step 001050): Train Loss 2.918 Val loss 6.058\n",
      "Ep 6 (Step 001055): Train Loss 3.193 Val loss 6.030\n",
      "Ep 6 (Step 001060): Train Loss 2.897 Val loss 6.061\n",
      "Ep 6 (Step 001065): Train Loss 2.911 Val loss 6.079\n",
      "Every effort moves you, and if I, that is true, I could have so annoying to me. I  “I am, ” said this, I have no   “I swear “I am you,\n",
      "Ep 7 (Step 001070): Train Loss 3.012 Val loss 6.085\n",
      "Ep 7 (Step 001075): Train Loss 2.999 Val loss 6.060\n",
      "Ep 7 (Step 001080): Train Loss 2.926 Val loss 6.110\n",
      "Ep 7 (Step 001085): Train Loss 2.851 Val loss 6.095\n",
      "Ep 7 (Step 001090): Train Loss 2.815 Val loss 5.998\n",
      "Ep 7 (Step 001095): Train Loss 2.969 Val loss 6.221\n",
      "Ep 7 (Step 001100): Train Loss 2.584 Val loss 6.283\n",
      "Ep 7 (Step 001105): Train Loss 2.437 Val loss 6.184\n",
      "Ep 7 (Step 001110): Train Loss 2.781 Val loss 6.341\n",
      "Ep 7 (Step 001115): Train Loss 2.693 Val loss 6.334\n",
      "Ep 7 (Step 001120): Train Loss 2.729 Val loss 6.159\n",
      "Ep 7 (Step 001125): Train Loss 2.854 Val loss 6.369\n",
      "Ep 7 (Step 001130): Train Loss 2.673 Val loss 6.337\n",
      "Ep 7 (Step 001135): Train Loss 2.371 Val loss 6.511\n",
      "Ep 7 (Step 001140): Train Loss 2.719 Val loss 6.256\n",
      "Ep 7 (Step 001145): Train Loss 2.630 Val loss 6.272\n",
      "Ep 7 (Step 001150): Train Loss 2.566 Val loss 6.404\n",
      "Ep 7 (Step 001155): Train Loss 2.763 Val loss 6.324\n",
      "Ep 7 (Step 001160): Train Loss 2.381 Val loss 6.266\n",
      "Ep 7 (Step 001165): Train Loss 2.703 Val loss 6.394\n",
      "Ep 7 (Step 001170): Train Loss 2.751 Val loss 6.317\n",
      "Ep 7 (Step 001175): Train Loss 2.193 Val loss 6.207\n",
      "Ep 7 (Step 001180): Train Loss 2.474 Val loss 6.361\n",
      "Ep 7 (Step 001185): Train Loss 2.340 Val loss 6.370\n",
      "Ep 7 (Step 001190): Train Loss 2.180 Val loss 6.344\n",
      "Ep 7 (Step 001195): Train Loss 2.638 Val loss 6.279\n",
      "Ep 7 (Step 001200): Train Loss 2.649 Val loss 6.204\n",
      "Ep 7 (Step 001205): Train Loss 2.606 Val loss 6.482\n",
      "Ep 7 (Step 001210): Train Loss 2.330 Val loss 6.197\n",
      "Ep 7 (Step 001215): Train Loss 2.331 Val loss 6.210\n",
      "Ep 7 (Step 001220): Train Loss 2.514 Val loss 6.349\n",
      "Ep 7 (Step 001225): Train Loss 2.436 Val loss 6.255\n",
      "Ep 7 (Step 001230): Train Loss 2.362 Val loss 6.360\n",
      "Ep 7 (Step 001235): Train Loss 2.423 Val loss 6.437\n",
      "Ep 7 (Step 001240): Train Loss 2.266 Val loss 6.346\n",
      "Ep 7 (Step 001245): Train Loss 2.343 Val loss 6.305\n",
      "Every effort moves you,  to be, and the world. I have       “The Turk of the “‘Do not to the morning when I        ‘Do not know\n",
      "Ep 8 (Step 001250): Train Loss 2.331 Val loss 6.282\n",
      "Ep 8 (Step 001255): Train Loss 2.115 Val loss 6.515\n",
      "Ep 8 (Step 001260): Train Loss 2.279 Val loss 6.339\n",
      "Ep 8 (Step 001265): Train Loss 1.996 Val loss 6.535\n",
      "Ep 8 (Step 001270): Train Loss 1.790 Val loss 6.545\n",
      "Ep 8 (Step 001275): Train Loss 1.693 Val loss 6.487\n",
      "Ep 8 (Step 001280): Train Loss 2.128 Val loss 6.544\n",
      "Ep 8 (Step 001285): Train Loss 2.181 Val loss 6.396\n",
      "Ep 8 (Step 001290): Train Loss 2.038 Val loss 6.550\n",
      "Ep 8 (Step 001295): Train Loss 1.897 Val loss 6.448\n",
      "Ep 8 (Step 001300): Train Loss 1.980 Val loss 6.581\n",
      "Ep 8 (Step 001305): Train Loss 2.098 Val loss 6.551\n",
      "Ep 8 (Step 001310): Train Loss 1.961 Val loss 6.520\n",
      "Ep 8 (Step 001315): Train Loss 2.018 Val loss 6.641\n",
      "Ep 8 (Step 001320): Train Loss 1.638 Val loss 6.720\n",
      "Ep 8 (Step 001325): Train Loss 2.021 Val loss 6.467\n",
      "Ep 8 (Step 001330): Train Loss 1.922 Val loss 6.720\n",
      "Ep 8 (Step 001335): Train Loss 2.101 Val loss 6.644\n",
      "Ep 8 (Step 001340): Train Loss 2.014 Val loss 6.589\n",
      "Ep 8 (Step 001345): Train Loss 1.726 Val loss 6.613\n",
      "Ep 8 (Step 001350): Train Loss 1.640 Val loss 6.571\n",
      "Ep 8 (Step 001355): Train Loss 1.576 Val loss 6.582\n",
      "Ep 8 (Step 001360): Train Loss 1.730 Val loss 6.540\n",
      "Ep 8 (Step 001365): Train Loss 1.750 Val loss 6.698\n",
      "Ep 8 (Step 001370): Train Loss 1.782 Val loss 6.634\n",
      "Ep 8 (Step 001375): Train Loss 1.824 Val loss 6.636\n",
      "Ep 8 (Step 001380): Train Loss 1.785 Val loss 6.484\n",
      "Ep 8 (Step 001385): Train Loss 1.448 Val loss 6.633\n",
      "Ep 8 (Step 001390): Train Loss 1.413 Val loss 6.724\n",
      "Ep 8 (Step 001395): Train Loss 1.441 Val loss 6.760\n",
      "Ep 8 (Step 001400): Train Loss 1.829 Val loss 6.619\n",
      "Ep 8 (Step 001405): Train Loss 1.615 Val loss 6.762\n",
      "Ep 8 (Step 001410): Train Loss 1.612 Val loss 6.530\n",
      "Ep 8 (Step 001415): Train Loss 1.606 Val loss 6.614\n",
      "Ep 8 (Step 001420): Train Loss 1.591 Val loss 6.702\n",
      "Every effort moves you, and you in case of the vulgar. I thought that I “I do not befall a sensitive being; I am your countenance which I do you to you. I ” “You are sorrowful,\n",
      "Ep 9 (Step 001425): Train Loss 1.406 Val loss 6.645\n",
      "Ep 9 (Step 001430): Train Loss 1.558 Val loss 6.747\n",
      "Ep 9 (Step 001435): Train Loss 1.531 Val loss 6.685\n",
      "Ep 9 (Step 001440): Train Loss 1.364 Val loss 6.973\n",
      "Ep 9 (Step 001445): Train Loss 1.301 Val loss 6.748\n",
      "Ep 9 (Step 001450): Train Loss 1.455 Val loss 7.071\n",
      "Ep 9 (Step 001455): Train Loss 1.109 Val loss 6.833\n",
      "Ep 9 (Step 001460): Train Loss 1.366 Val loss 6.868\n",
      "Ep 9 (Step 001465): Train Loss 1.218 Val loss 6.753\n",
      "Ep 9 (Step 001470): Train Loss 1.305 Val loss 6.900\n",
      "Ep 9 (Step 001475): Train Loss 0.971 Val loss 6.829\n",
      "Ep 9 (Step 001480): Train Loss 1.314 Val loss 6.863\n",
      "Ep 9 (Step 001485): Train Loss 1.058 Val loss 6.924\n",
      "Ep 9 (Step 001490): Train Loss 1.284 Val loss 6.890\n",
      "Ep 9 (Step 001495): Train Loss 1.001 Val loss 6.856\n",
      "Ep 9 (Step 001500): Train Loss 1.013 Val loss 6.942\n",
      "Ep 9 (Step 001505): Train Loss 1.145 Val loss 6.955\n",
      "Ep 9 (Step 001510): Train Loss 1.274 Val loss 6.982\n",
      "Ep 9 (Step 001515): Train Loss 1.198 Val loss 6.928\n",
      "Ep 9 (Step 001520): Train Loss 1.134 Val loss 6.959\n",
      "Ep 9 (Step 001525): Train Loss 1.116 Val loss 6.796\n",
      "Ep 9 (Step 001530): Train Loss 1.136 Val loss 6.757\n",
      "Ep 9 (Step 001535): Train Loss 1.015 Val loss 6.935\n",
      "Ep 9 (Step 001540): Train Loss 0.994 Val loss 7.100\n",
      "Ep 9 (Step 001545): Train Loss 1.186 Val loss 6.879\n",
      "Ep 9 (Step 001550): Train Loss 0.798 Val loss 7.005\n",
      "Ep 9 (Step 001555): Train Loss 1.087 Val loss 6.911\n",
      "Ep 9 (Step 001560): Train Loss 0.988 Val loss 6.902\n",
      "Ep 9 (Step 001565): Train Loss 1.285 Val loss 6.785\n",
      "Ep 9 (Step 001570): Train Loss 1.092 Val loss 7.020\n",
      "Ep 9 (Step 001575): Train Loss 1.003 Val loss 7.022\n",
      "Ep 9 (Step 001580): Train Loss 0.893 Val loss 7.037\n",
      "Ep 9 (Step 001585): Train Loss 0.954 Val loss 7.145\n",
      "Ep 9 (Step 001590): Train Loss 1.017 Val loss 7.129\n",
      "Ep 9 (Step 001595): Train Loss 1.020 Val loss 6.971\n",
      "Ep 9 (Step 001600): Train Loss 0.989 Val loss 7.039\n",
      "Every effort moves you belong then to be happy is that his peaceful mind, the dark about the profits of the a he had been gone. He endeavoured to the cause of his condemnation. The    “I can I have no money\n",
      "Ep 10 (Step 001605): Train Loss 0.871 Val loss 6.946\n",
      "Ep 10 (Step 001610): Train Loss 0.798 Val loss 6.950\n",
      "Ep 10 (Step 001615): Train Loss 0.763 Val loss 7.141\n",
      "Ep 10 (Step 001620): Train Loss 0.872 Val loss 6.937\n",
      "Ep 10 (Step 001625): Train Loss 0.854 Val loss 7.154\n",
      "Ep 10 (Step 001630): Train Loss 0.929 Val loss 7.200\n",
      "Ep 10 (Step 001635): Train Loss 0.666 Val loss 7.262\n",
      "Ep 10 (Step 001640): Train Loss 0.822 Val loss 7.176\n",
      "Ep 10 (Step 001645): Train Loss 0.765 Val loss 7.006\n",
      "Ep 10 (Step 001650): Train Loss 0.786 Val loss 7.308\n",
      "Ep 10 (Step 001655): Train Loss 0.760 Val loss 7.375\n",
      "Ep 10 (Step 001660): Train Loss 0.744 Val loss 7.088\n",
      "Ep 10 (Step 001665): Train Loss 0.914 Val loss 7.203\n",
      "Ep 10 (Step 001670): Train Loss 0.841 Val loss 7.183\n",
      "Ep 10 (Step 001675): Train Loss 0.673 Val loss 7.362\n",
      "Ep 10 (Step 001680): Train Loss 0.576 Val loss 7.350\n",
      "Ep 10 (Step 001685): Train Loss 0.699 Val loss 7.092\n",
      "Ep 10 (Step 001690): Train Loss 0.713 Val loss 7.221\n",
      "Ep 10 (Step 001695): Train Loss 0.702 Val loss 7.196\n",
      "Ep 10 (Step 001700): Train Loss 0.668 Val loss 7.426\n",
      "Ep 10 (Step 001705): Train Loss 0.651 Val loss 7.416\n",
      "Ep 10 (Step 001710): Train Loss 0.613 Val loss 7.276\n",
      "Ep 10 (Step 001715): Train Loss 0.737 Val loss 7.376\n",
      "Ep 10 (Step 001720): Train Loss 0.627 Val loss 7.269\n",
      "Ep 10 (Step 001725): Train Loss 0.794 Val loss 7.250\n",
      "Ep 10 (Step 001730): Train Loss 0.655 Val loss 7.334\n",
      "Ep 10 (Step 001735): Train Loss 0.579 Val loss 7.282\n",
      "Ep 10 (Step 001740): Train Loss 0.625 Val loss 7.094\n",
      "Ep 10 (Step 001745): Train Loss 0.606 Val loss 7.143\n",
      "Ep 10 (Step 001750): Train Loss 0.611 Val loss 7.109\n",
      "Ep 10 (Step 001755): Train Loss 0.527 Val loss 7.311\n",
      "Ep 10 (Step 001760): Train Loss 0.481 Val loss 7.478\n",
      "Ep 10 (Step 001765): Train Loss 0.561 Val loss 7.215\n",
      "Ep 10 (Step 001770): Train Loss 0.502 Val loss 7.488\n",
      "Ep 10 (Step 001775): Train Loss 0.511 Val loss 7.363\n",
      "Every effort moves you belong would only impress more deeply.’  I have been three months ago, and then you are my fellow I need by my fate I love the inconveniences and perhaps dangers of knowledge of task is already fulfilled!’ \n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    device, \n",
    "    num_epochs=num_epochs, \n",
    "    eval_freq=5, \n",
    "    eval_iter=5, \n",
    "    start_context=start_context,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35m 30.4s\n",
    "# 20m 50.1s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
